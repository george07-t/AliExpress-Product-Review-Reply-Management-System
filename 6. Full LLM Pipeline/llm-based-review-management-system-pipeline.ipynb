{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65XlP9BBkWL6"
   },
   "source": [
    "## Import Unsloth Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-16T06:40:06.673299Z",
     "iopub.status.busy": "2025-04-16T06:40:06.67304Z",
     "iopub.status.idle": "2025-04-16T06:43:09.117621Z",
     "shell.execute_reply": "2025-04-16T06:43:09.116827Z",
     "shell.execute_reply.started": "2025-04-16T06:40:06.673279Z"
    },
    "id": "0HZjD-sUkWL9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install unsloth\n",
    "# Get latest Unsloth\n",
    "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-16T06:45:10.672406Z",
     "iopub.status.busy": "2025-04-16T06:45:10.671785Z",
     "iopub.status.idle": "2025-04-16T06:45:16.394116Z",
     "shell.execute_reply": "2025-04-16T06:45:16.393152Z",
     "shell.execute_reply.started": "2025-04-16T06:45:10.672383Z"
    },
    "id": "9k0gOoY1kWL-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d5bf1d31-960e-40bf-b0ee-f132d693989c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=418df58fc6b4def8a54569e3e4c24ee644fb6cfdafafc04896df65bbfb30a736\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score, evaluate, bert-score\n",
      "Successfully installed bert-score-0.3.13 evaluate-0.4.3 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate bert-score nltk rouge_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSN93xSskWL_"
   },
   "source": [
    "## Function to Load the Fine-tunned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T06:43:09.119633Z",
     "iopub.status.busy": "2025-04-16T06:43:09.119382Z",
     "iopub.status.idle": "2025-04-16T06:43:43.929489Z",
     "shell.execute_reply": "2025-04-16T06:43:43.928703Z",
     "shell.execute_reply.started": "2025-04-16T06:43:09.119612Z"
    },
    "id": "UcwSPIoskWMA",
    "outputId": "82491192-e723-402d-8500-55717b3cd2a1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 06:43:20.917589: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744785801.165144      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744785801.238449      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "def load_unsloth_model(model_name):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vbPvO9jkWMD"
   },
   "source": [
    "## Pipeline Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:47:38.358868Z",
     "iopub.status.busy": "2025-04-16T08:47:38.358050Z",
     "iopub.status.idle": "2025-04-16T08:47:38.362146Z",
     "shell.execute_reply": "2025-04-16T08:47:38.361386Z",
     "shell.execute_reply.started": "2025-04-16T08:47:38.358842Z"
    },
    "id": "xjrfWEAikWMD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "review = \"I do not understand the enthusiastic reviews on the basis of which I ordered this bag. Looks very cheap. As the skin of a young dermatine is said.\"\n",
    "input_category = \"health\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJw1sDQYkWMD"
   },
   "source": [
    "## First Phase of Pipeline\n",
    "\n",
    "- Load `AbuSalehMd/FakeReviewDetection_Mistral_7B_FineTuned` & `AbuSalehMd/ProductCategoryClassificationFinal_Mistral_7B_FineTuned` for detecting fake review and classify product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d5c0d9e75639441398b126ffd38cf31d",
      "3a2d58d5f46743ef8bdae20c65a623b6",
      "214c391b61f3424e8341c06306e6b730",
      "4a4870fe88bf457da67c938d3c0abe06",
      "22703e16b2984d2b9c714fc04cc981ac",
      "c98eb4e2623c4e18a9a7664736a2ceae",
      "387ddc83f97941b5b3c645ba4e1e9994",
      "356cedbe3e9a4848adf4c9449d612175"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-04-16T08:50:27.191592Z",
     "iopub.status.busy": "2025-04-16T08:50:27.191064Z",
     "iopub.status.idle": "2025-04-16T08:50:51.605884Z",
     "shell.execute_reply": "2025-04-16T08:50:51.605301Z",
     "shell.execute_reply.started": "2025-04-16T08:50:27.191570Z"
    },
    "id": "PCdSkJk5kWMD",
    "outputId": "1bbe5045-a94f-4a34-f47a-d3c89105f3f4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Assign models to specific GPUs\n",
    "model_1, tokenizer_1 = load_unsloth_model(\"AbuSalehMd/FakeReviewDetection_Mistral_7B_FineTuned\")\n",
    "model_2, tokenizer_2 = load_unsloth_model(\"AbuSalehMd/ProductCategoryClassificationFinal_Mistral_7B_FineTuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWQd_QLFkWME"
   },
   "source": [
    "## Prediction Functions for Fake Review and Product Category\n",
    "\n",
    "- Defines prompt-based inference functions to detect whether a review is *real or fake* and to classify its *product category*.\n",
    "- Uses a custom prompt format and Unsloth's `FastLanguageModel` for generating model outputs.\n",
    "- Handles prediction post-processing to extract clean labels like \"real\", \"fake\", or one of five categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:51:06.891187Z",
     "iopub.status.busy": "2025-04-16T08:51:06.890444Z",
     "iopub.status.idle": "2025-04-16T08:51:06.896591Z",
     "shell.execute_reply": "2025-04-16T08:51:06.895849Z",
     "shell.execute_reply.started": "2025-04-16T08:51:06.891163Z"
    },
    "id": "Dk-qobNdkWME",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_test_prompt_single_fake(review_text):\n",
    "    return f\"\"\"\n",
    "    Determine if the review enclosed in square brackets is real or fake based on its content.\n",
    "    Return the answer as either \"real\" or \"fake\".\n",
    "\n",
    "    [{review_text}] =\n",
    "    \"\"\".strip()\n",
    "\n",
    "def predict_fake_review(review_text, model, tokenizer):\n",
    "    from unsloth import FastLanguageModel\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    prompt = generate_test_prompt_single_fake(review_text)\n",
    "\n",
    "    # Make sure tensor goes to the correct GPU/device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=1,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    answer = result.split(\"=\")[-1].strip().lower()\n",
    "\n",
    "    if \"real\" in answer:\n",
    "        return \"real\"\n",
    "    elif \"fake\" in answer:\n",
    "        return \"fake\"\n",
    "    else:\n",
    "        return \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:51:09.876500Z",
     "iopub.status.busy": "2025-04-16T08:51:09.875919Z",
     "iopub.status.idle": "2025-04-16T08:51:09.882165Z",
     "shell.execute_reply": "2025-04-16T08:51:09.881472Z",
     "shell.execute_reply.started": "2025-04-16T08:51:09.876478Z"
    },
    "id": "JbgoczwWkWME",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_test_prompt_single_product(review_text):\n",
    "    return f\"\"\"\n",
    "    Determine the class if the review enclosed in square brackets is automotive or fashion or home or electronics or health category class based on its content.\n",
    "    Return the answer as either \"automotive\" or \"fashion\" or \"home\" or \"electronics\" or \"health\".\n",
    "\n",
    "    [{review_text}] =\n",
    "    \"\"\".strip()\n",
    "\n",
    "def predict_category(review_text, model, tokenizer):\n",
    "    from unsloth import FastLanguageModel\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    prompt = generate_test_prompt_single_product(review_text)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1, temperature=0.0)\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    answer = result.split(\"=\")[-1].strip().lower()\n",
    "    if \"autom\" in answer:\n",
    "        return \"automotive\"\n",
    "    elif \"fashion\" in answer:\n",
    "        return \"fashion\"\n",
    "    elif \"home\" in answer:\n",
    "        return \"home\"\n",
    "    elif \"electron\" in answer:\n",
    "        return \"electronics\"\n",
    "    elif \"health\" in answer:\n",
    "        return \"health\"\n",
    "    else:\n",
    "        return \"none\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW5wFUlskWME"
   },
   "source": [
    "## Fake Review and Category Detection Pipeline\n",
    "\n",
    "- Implements a 2-step pipeline to first detect *fake reviews*, then verify if the predicted *category* matches the input.\n",
    "- Automatically filters out irrelevant or fake reviews before proceeding further.\n",
    "- Logs time taken to run the full pipeline for performance evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:51:13.160709Z",
     "iopub.status.busy": "2025-04-16T08:51:13.160079Z",
     "iopub.status.idle": "2025-04-16T08:51:13.165410Z",
     "shell.execute_reply": "2025-04-16T08:51:13.164756Z",
     "shell.execute_reply.started": "2025-04-16T08:51:13.160684Z"
    },
    "id": "7FUytbDDkWME",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def first_pipeline(review,input_category):\n",
    "    print(f\"üîç Review: {review}\\nüì¶ Input Category: {input_category}\")\n",
    "\n",
    "    # Step 1: Fake Review Detection\n",
    "    fake_result = predict_fake_review(review, model_1, tokenizer_1)\n",
    "    print(\"üïµÔ∏è Fake Review Detection:\", fake_result)\n",
    "\n",
    "    if fake_result == \"fake\":\n",
    "        print(\"‚ùå Detected as fake review.\")\n",
    "        return \"Fake\"\n",
    "    else:\n",
    "        # Step 2: Category Classification\n",
    "        category = predict_category(review, model_2, tokenizer_2)\n",
    "        print(\"üè∑Ô∏è Predicted Category:\", category)\n",
    "        if category != input_category.lower():\n",
    "            print(\"‚ö†Ô∏è Irrelevant category.\")\n",
    "            return \"Irrelevant\"\n",
    "        else:\n",
    "            return \"Rrelevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:51:18.578879Z",
     "iopub.status.busy": "2025-04-16T08:51:18.578168Z",
     "iopub.status.idle": "2025-04-16T08:51:19.258520Z",
     "shell.execute_reply": "2025-04-16T08:51:19.257842Z",
     "shell.execute_reply.started": "2025-04-16T08:51:18.578856Z"
    },
    "id": "04bKCqK9kWME",
    "outputId": "5a02120c-225f-4f3a-e3dc-0112209f668d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Review: I do not understand the enthusiastic reviews on the basis of which I ordered this bag. Looks very cheap. As the skin of a young dermatine is said.\n",
      "üì¶ Input Category: health\n",
      "üïµÔ∏è Fake Review Detection: real\n",
      "üè∑Ô∏è Predicted Category: health\n",
      "\n",
      "‚è±Ô∏è Time taken 1st Pipeline: 0.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run your pipeline\n",
    "output1 = first_pipeline(review, input_category)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print duration\n",
    "duration1 = end_time - start_time\n",
    "print(f\"\\n‚è±Ô∏è Time taken 1st Pipeline: {duration1:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENCp7M1RkWMF"
   },
   "source": [
    "## Second Phase of Pipeline\n",
    "\n",
    "- load `AbuSalehMd/Review_Response_Generation_Mistral_7B_FineTuned` & `AbuSalehMd/Sentiment_Analysis_Mistral_7B_FineTuned` for sentiment and response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:52:26.321936Z",
     "iopub.status.busy": "2025-04-16T08:52:26.321612Z",
     "iopub.status.idle": "2025-04-16T08:52:49.450054Z",
     "shell.execute_reply": "2025-04-16T08:52:49.449258Z",
     "shell.execute_reply.started": "2025-04-16T08:52:26.321916Z"
    },
    "id": "l1m8w_oFkWMF",
    "outputId": "a490ac6d-06aa-4ae4-e956-56c71654f75b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Unload both models after loop\n",
    "del model_1, model_2, tokenizer_1, tokenizer_2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_1, tokenizer_1 = load_unsloth_model(\"AbuSalehMd/Review_Response_Generation_Mistral_7B_FineTuned\")\n",
    "model_2, tokenizer_2 = load_unsloth_model(\"AbuSalehMd/Sentiment_Analysis_Mistral_7B_FineTuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Etxl5BfUkWMF"
   },
   "source": [
    "## Sentiment Classification and Reply Generation\n",
    "\n",
    "- `predict_sentiment`: Classifies a review as *positive*, *neutral*, or *negative* using a prompt-based method.\n",
    "- `generate_review_reply`: Produces a personalized reply based on the review content, its sentiment, and category using the Alpaca prompt format.\n",
    "- Ensures the response is contextually relevant and informative for end users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:52:53.558993Z",
     "iopub.status.busy": "2025-04-16T08:52:53.558704Z",
     "iopub.status.idle": "2025-04-16T08:52:53.564098Z",
     "shell.execute_reply": "2025-04-16T08:52:53.563483Z",
     "shell.execute_reply.started": "2025-04-16T08:52:53.558976Z"
    },
    "id": "dB0LLQ5PkWMF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(review_text, model, tokenizer):\n",
    "    from unsloth import FastLanguageModel\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Determine if the review enclosed in square brackets is positive, neutral or negative based on its content.\n",
    "    Return the answer as either \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "    [{review_text}] =\n",
    "    \"\"\".strip()\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1, temperature=0.0)\n",
    "    answer = tokenizer.decode(outputs[0]).split(\"=\")[-1].strip().lower()\n",
    "\n",
    "    if \"positive\" in answer:\n",
    "        return \"positive\"\n",
    "    elif \"neutral\" in answer:\n",
    "        return \"neutral\"\n",
    "    elif \"negative\" in answer:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:52:56.262635Z",
     "iopub.status.busy": "2025-04-16T08:52:56.262053Z",
     "iopub.status.idle": "2025-04-16T08:52:56.269120Z",
     "shell.execute_reply": "2025-04-16T08:52:56.268374Z",
     "shell.execute_reply.started": "2025-04-16T08:52:56.262607Z"
    },
    "id": "avHAb_2TkWMF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the prompt format\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def generate_review_reply(review, sentiment, category_label, model, tokenizer, alpaca_prompt):\n",
    "    from unsloth import FastLanguageModel\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = alpaca_prompt.format(\n",
    "        \"Generate a helpful and context-aware reply based on the review, sentiment, and category.\",\n",
    "        f\"Review: {review}\\nSentiment: {sentiment}\\nCategory: {category_label}\",\n",
    "        \"\"\n",
    "    )\n",
    "\n",
    "    # Tokenize and move inputs to the correct device\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "\n",
    "    # Decode output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the generated reply\n",
    "    if \"### Response:\" in decoded_output:\n",
    "        reply = decoded_output.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        reply = decoded_output.strip()\n",
    "\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQCXelHSkWMF"
   },
   "source": [
    "## Sentiment & Reply Generation Pipeline with Timing\n",
    "\n",
    "- Runs sentiment classification and reply generation only if the review passes fake detection and category matching.\n",
    "- Measures and prints execution time for Part 2 as well as the total time for the full pipeline.\n",
    "- Ensures that responses are only generated for valid, real, and category-relevant reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:52:59.119451Z",
     "iopub.status.busy": "2025-04-16T08:52:59.119172Z",
     "iopub.status.idle": "2025-04-16T08:52:59.123873Z",
     "shell.execute_reply": "2025-04-16T08:52:59.123144Z",
     "shell.execute_reply.started": "2025-04-16T08:52:59.119431Z"
    },
    "id": "A8rN2pEvkWMF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def second_pipeline(review,input_category):\n",
    "    # Step 3: Sentiment Classification\n",
    "    sentiment = predict_sentiment(review, model_2, tokenizer_2)\n",
    "    print(\"üí¨ Sentiment:\", sentiment)\n",
    "\n",
    "    # Step 4: Generate Reply\n",
    "    reply = generate_review_reply(review, sentiment, input_category, model_1, tokenizer_1, alpaca_prompt)\n",
    "    print(\"‚úçÔ∏è Generated Reply:\")\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:53:01.974395Z",
     "iopub.status.busy": "2025-04-16T08:53:01.974122Z",
     "iopub.status.idle": "2025-04-16T08:53:04.918612Z",
     "shell.execute_reply": "2025-04-16T08:53:04.917966Z",
     "shell.execute_reply.started": "2025-04-16T08:53:01.974376Z"
    },
    "id": "06Wxf0P1kWMG",
    "outputId": "37d2ae4b-5358-400d-c62d-7bc4ad1b1bac",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Sentiment: negative\n",
      "‚úçÔ∏è Generated Reply:\n",
      "‚úÖ Final Output (Reply): Thank you for your feedback! We appreciate your honesty and understand that the product didn't meet your expectations. We are constantly working to improve and offer more options that may be a better fit.\n",
      "\n",
      "‚è±Ô∏è Total Time Taken 2nd Pipeline: 2.94 seconds\n",
      "\n",
      "‚è±Ô∏è Total Time for Full Pipeline: 3.62 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start total timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run part 2 only if relevant\n",
    "if output1 == \"Rrelevant\":\n",
    "    output2 = second_pipeline(review, input_category)\n",
    "    print(\"‚úÖ Final Output (Reply):\", output2)\n",
    "else:\n",
    "    print(\"üö´ Final Output:\", output1)\n",
    "\n",
    "# End total timer\n",
    "end_time = time.time()\n",
    "duration2 = end_time - start_time\n",
    "# Show execution duration\n",
    "print(f\"\\n‚è±Ô∏è Total Time Taken 2nd Pipeline: {duration2:.2f} seconds\")\n",
    "\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Total Time for Full Pipeline: {duration1+duration2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:50:18.430266Z",
     "iopub.status.busy": "2025-04-16T08:50:18.429958Z",
     "iopub.status.idle": "2025-04-16T08:50:19.458552Z",
     "shell.execute_reply": "2025-04-16T08:50:19.457981Z",
     "shell.execute_reply.started": "2025-04-16T08:50:18.430244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Unload both models after loop\n",
    "del model_1, model_2, tokenizer_1, tokenizer_2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset for Full Inference Pipeline for Evaluation\n",
    "\n",
    "- Contains 14 labeled review samples with expected outputs for fake review detection, product category classification, sentiment analysis, and reply generation.\n",
    "- Each entry includes:\n",
    "  - `review`: User review text.\n",
    "  - `input_category`: User-provided product category.\n",
    "  - `expected_fake`, `expected_category`, `expected_sentiment`, `expected_reply`: Expected outputs for evaluation.\n",
    "- Out of 14 examples, **10 are valid** (i.e., real and correctly categorized) and passed to the second pipeline stage for sentiment and reply generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:37:03.455640Z",
     "iopub.status.busy": "2025-04-16T08:37:03.455304Z",
     "iopub.status.idle": "2025-04-16T08:37:03.465091Z",
     "shell.execute_reply": "2025-04-16T08:37:03.464243Z",
     "shell.execute_reply.started": "2025-04-16T08:37:03.455618Z"
    },
    "id": "Ctp28rIbkWMA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"review\": \"Very satisfied with the product, it is really quite strong. Only bad point is that it does not turn on any light during the Batt's carrageway to know if the load is complete with wonderful rest, I recommend!\",\n",
    "        \"input_category\": \"automotive\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"automotive\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"That's great to hear! Strong and reliable products are always appreciated. Thanks for the positive note!\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"I really liked I can take out my quiet dog\",\n",
    "        \"input_category\": \"fashion\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"fashion\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"That's great to hear! Thanks for the positive note and your creative use of the product.\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"nice nice nice nice nice\",\n",
    "        \"input_category\": \"fashion\",\n",
    "        \"expected_fake\": \"fake\",\n",
    "        \"expected_category\": \"fashion\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"These seat covers fit perfectly in my car. They are very easy to install and look great!\",\n",
    "        \"input_category\": \"automotive\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"automotive\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"Perfect fit is always a good sign! Thanks for the great review and your support.\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"Really high-quality bags, buying worth...\",\n",
    "        \"input_category\": \"fashion\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"fashion\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"That's great to hear! Thanks for the positive note and your support.\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"Order received successfully and faster than expected. The covers are as described. Perfect size and suitable color. Very satisfied with the purchase.\",\n",
    "        \"input_category\": \"electronics\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"electronics\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"That‚Äôs always great news‚Äîthanks for the note!\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"I like Xro for my sofa no m served quality fabric\",\n",
    "        \"input_category\": \"home\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"home\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"Thanks for the great feedback! We are glad the quality and fit met your expectations.\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"The FIta or belt is thin because the price is already good.\",\n",
    "        \"input_category\": \"health\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"health\",\n",
    "        \"expected_sentiment\": \"neutral\",\n",
    "        \"expected_reply\": \"Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your expectations. We are always working to improve, so feel free to check out our other models.\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"Delivery Latvia 10 days. Original case, sound, aplication good\",\n",
    "        \"input_category\": \"electronics\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"electronics\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your full expectations. We are always working to improve, so feel free to check out our other models.\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"The vacuum cleaner broke down after only two weeks. I am very disappointed with the quality.\",\n",
    "        \"input_category\": \"home\",\n",
    "        \"expected_fake\": \"real\",\n",
    "        \"expected_category\": \"home\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "        \"expected_reply\": \"We‚Äôre sorry for your experience. Please reach out to our support so we can help resolve this issue for you.\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"Good product. Very good. I like. Will buy again. Good product. Very good.\",\n",
    "        \"input_category\": \"electronics\",\n",
    "        \"expected_fake\": \"fake\",\n",
    "        \"expected_category\": \"electronics\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"thank you all the dependable\",\n",
    "        \"input_category\": \"automotive\",\n",
    "        \"expected_fake\": \"fake\",\n",
    "        \"expected_category\": \"automotive\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"super quality and bistra ane shipping\",\n",
    "        \"input_category\": \"health\",\n",
    "        \"expected_fake\": \"fake\",\n",
    "        \"expected_category\": \"health\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"the case is great a the picture the speech speed is also excellent\",\n",
    "        \"input_category\": \"home\",\n",
    "        \"expected_fake\": \"fake\",\n",
    "        \"expected_category\": \"home\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "        \"expected_reply\": \"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "passed_examples=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 & 2: Fake Review Detection and Category Classification\n",
    "\n",
    "- Loads and applies fine-tuned Unsloth models for fake review detection and product category classification.\n",
    "- Immediately filters out reviews predicted as **fake** before proceeding to category classification.\n",
    "- Classifies remaining reviews into one of five base categories and filters out mismatches with the input category.\n",
    "- Stores only valid and relevant reviews for sentiment analysis and reply generation in Part 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:37:13.381064Z",
     "iopub.status.busy": "2025-04-16T08:37:13.380470Z",
     "iopub.status.idle": "2025-04-16T08:37:44.932836Z",
     "shell.execute_reply": "2025-04-16T08:37:44.931994Z",
     "shell.execute_reply.started": "2025-04-16T08:37:13.381040Z"
    },
    "id": "QknffIU2kWMB",
    "outputId": "c4402d42-8be7-4efc-ec41-5af9fe8b241a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "üìã Step 1 & 2 Predictions:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Sample 1\n",
      "üìù Review: Very satisfied with the product, it is really quite strong...\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   automotive  | Input Category: automotive\n",
      "‚úÖ Passed to Part 2\n",
      "\n",
      "üîÅ Sample 2\n",
      "üìù Review: I really liked I can take out my quiet dog\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   fashion  | Input Category: fashion\n",
      "‚úÖ Passed to Part 2\n",
      "\n",
      "üîÅ Sample 3\n",
      "üìù Review: nice nice nice nice nice\n",
      "üîç Predicted Fake/Real: fake | Expected: fake\n",
      "‚ùå Filtered out before Part 2 (Fake Review)\n",
      "\n",
      "üîÅ Sample 4\n",
      "üìù Review: These seat covers fit perfectly in my car...\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   automotive  | Input Category: automotive\n",
      "‚úÖ Passed to Part 2\n",
      "\n",
      "üîÅ Sample 5\n",
      "üìù Review: Really high-quality bags, buying worth...\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   fashion  | Input Category: fashion\n",
      "‚úÖ Passed to Part 2\n",
      "\n",
      "üîÅ Sample 6\n",
      "üìù Review: Order received successfully and faster than expected...\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   electronics  | Input Category: electronics\n",
      "‚úÖ Passed to Part 2\n",
      "\n",
      "üîÅ Sample 7\n",
      "üìù Review: I like Xro for my sofa no m served quality fabric\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   home  | Input Category: home\n",
      "‚úÖ Passed to Part 2\n",
      "\n",
      "üîÅ Sample 8\n",
      "üìù Review: The FIta or belt is thin because the price is already good.\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   health  | Input Category: health\n",
      "‚úÖ Passed to Part 2\n",
      "\n",
      "üîÅ Sample 9\n",
      "üìù Review: Delivery Latvia 10 days. Original case, sound, aplication good\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   electronics  | Input Category: electronics\n",
      "‚úÖ Passed to Part 2\n",
      "\n",
      "üîÅ Sample 10\n",
      "üìù Review: The vacuum cleaner broke down after only two weeks...\n",
      "üîç Predicted Fake/Real: real | Expected: real\n",
      "üè∑Ô∏è Predicted Category:   electronics  | Input Category: home\n",
      "‚ùå Filtered out before Part 2 (Category Mismatch)\n",
      "\n",
      "üîÅ Sample 11\n",
      "üìù Review: Good product. Very good. I like. Will buy again. Good product. Very good.\n",
      "üîç Predicted Fake/Real: fake | Expected: fake\n",
      "‚ùå Filtered out before Part 2 (Fake Review)\n",
      "\n",
      "üîÅ Sample 12\n",
      "üìù Review: thank you all the dependable\n",
      "üîç Predicted Fake/Real: fake | Expected: fake\n",
      "‚ùå Filtered out before Part 2 (Fake Review)\n",
      "\n",
      "üîÅ Sample 13\n",
      "üìù Review: super quality and bistra ane shipping\n",
      "üîç Predicted Fake/Real: fake | Expected: fake\n",
      "‚ùå Filtered out before Part 2 (Fake Review)\n",
      "\n",
      "üîÅ Sample 14\n",
      "üìù Review: the case is great a the picture the speech speed is also excellent\n",
      "üîç Predicted Fake/Real: fake | Expected: fake\n",
      "‚ùå Filtered out before Part 2 (Fake Review)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Step 1 & 2 storage\n",
    "fake_preds, fake_labels = [], []\n",
    "cat_preds, cat_labels = [], []\n",
    "\n",
    "# This will hold items that pass both steps for Part 2\n",
    "passed_samples = []\n",
    "\n",
    "# ‚úÖ Load models once before loop\n",
    "model_1, tokenizer_1 = load_unsloth_model(\"AbuSalehMd/FakeReviewDetection_Mistral_7B_FineTuned\")\n",
    "model_2, tokenizer_2 = load_unsloth_model(\"AbuSalehMd/ProductCategoryClassificationFinal_Mistral_7B_FineTuned\")\n",
    "\n",
    "print(\"\\nüìã Step 1 & 2 Predictions:\")\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "\n",
    "for idx, item in enumerate(test_data):\n",
    "    review = item[\"review\"]\n",
    "    input_category = item[\"input_category\"]\n",
    "\n",
    "    # Step 1: Fake detection\n",
    "    fake = predict_fake_review(review, model_1, tokenizer_1)\n",
    "    fake_preds.append(fake)\n",
    "    fake_labels.append(item[\"expected_fake\"])\n",
    "\n",
    "    # If fake, reject immediately\n",
    "    if fake == \"fake\":\n",
    "        print(f\"\\nüîÅ Sample {idx+1}\")\n",
    "        print(f\"üìù Review: {review}\")\n",
    "        print(f\"üîç Predicted Fake/Real: {fake} | Expected: {item['expected_fake']}\")\n",
    "        print(f\"‚ùå Filtered out before Part 2 (Fake Review)\")\n",
    "        continue\n",
    "\n",
    "    # Step 2: Category classification\n",
    "    cat = predict_category(review, model_2, tokenizer_2)\n",
    "    cat_preds.append(cat)\n",
    "    cat_labels.append(item[\"expected_category\"])\n",
    "\n",
    "    # üìä Print outcomes\n",
    "    print(f\"\\nüîÅ Sample {idx+1}\")\n",
    "    print(f\"üìù Review: {review}\")\n",
    "    print(f\"üîç Predicted Fake/Real: {fake} | Expected: {item['expected_fake']}\")\n",
    "    print(f\"üè∑Ô∏è Predicted Category:   {cat}  | Input Category: {input_category}\")\n",
    "\n",
    "    # Check if predicted category matches input category\n",
    "    if cat == input_category:\n",
    "        passed_samples.append({\n",
    "            \"review\": review,\n",
    "            \"category\": cat,\n",
    "            \"expected_sentiment\": item[\"expected_sentiment\"],\n",
    "            \"expected_reply\": item[\"expected_reply\"]\n",
    "        })\n",
    "        print(\"‚úÖ Passed to Part 2\")\n",
    "    else:\n",
    "        print(\"‚ùå Filtered out before Part 2 (Category Mismatch)\")\n",
    "\n",
    "# ‚úÖ Unload both models after loop\n",
    "del model_1, model_2, tokenizer_1, tokenizer_2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 & 4: Sentiment Analysis and Reply Generation\n",
    "\n",
    "- Loads fine-tuned models for sentiment classification and context-aware reply generation.\n",
    "- Predicts the **sentiment** (positive, neutral, negative) for each review passed from previous stages.\n",
    "- Generates a helpful and personalized **reply** using a structured Alpaca-style prompt format.\n",
    "- Compares predicted sentiment and reply against expected labels for evaluation and inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:37:44.934937Z",
     "iopub.status.busy": "2025-04-16T08:37:44.934704Z",
     "iopub.status.idle": "2025-04-16T08:38:22.409141Z",
     "shell.execute_reply": "2025-04-16T08:38:22.408565Z",
     "shell.execute_reply.started": "2025-04-16T08:37:44.934920Z"
    },
    "id": "KiAXO6NFkWMC",
    "outputId": "903f3bda-35de-448c-e023-6bd6e026d8b4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "üìã Step 3 & 4 Predictions (for passed samples):\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Passed Sample 1\n",
      "üìù Review: Very satisfied with the product, it is really quite strong...\n",
      "üí¨ Predicted Sentiment: positive | Expected: positive\n",
      "üìù Generated Reply: That's great to hear! Strong and reliable products are always appreciated. Thanks for the positive note!\n",
      "‚úÖ Expected Reply:  That's great to hear! Strong and reliable products are always appreciated. Thanks for the positive note!\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Passed Sample 2\n",
      "üìù Review: I really liked I can take out my quiet dog\n",
      "üí¨ Predicted Sentiment: positive | Expected: positive\n",
      "üìù Generated Reply: That's great to hear! Thanks for the positive note and your creative use of the product.\n",
      "‚úÖ Expected Reply:  That's great to hear! Thanks for the positive note and your creative use of the product.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Passed Sample 3\n",
      "üìù Review: These seat covers fit perfectly in my car...\n",
      "üí¨ Predicted Sentiment: positive | Expected: positive\n",
      "üìù Generated Reply: Perfect fit is always a good sign! Thanks for the great review and your support.\n",
      "‚úÖ Expected Reply:  Perfect fit is always a good sign! Thanks for the great review and your support.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Passed Sample 4\n",
      "üìù Review: Really high-quality bags, buying worth...\n",
      "üí¨ Predicted Sentiment: positive | Expected: positive\n",
      "üìù Generated Reply: That's great to hear! Thanks for the positive note and your support.\n",
      "‚úÖ Expected Reply:  That's great to hear! Thanks for the positive note and your support.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Passed Sample 5\n",
      "üìù Review: Order received successfully and faster than expected...\n",
      "üí¨ Predicted Sentiment: positive | Expected: positive\n",
      "üìù Generated Reply: That¬ís always great news¬óthanks for the note!\n",
      "‚úÖ Expected Reply:  That‚Äôs always great news‚Äîthanks for the note!\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Passed Sample 6\n",
      "üìù Review: I like Xro for my sofa no m served quality fabric\n",
      "üí¨ Predicted Sentiment: positive | Expected: positive\n",
      "üìù Generated Reply: Thanks for the great feedback! We are glad the quality and fit met your expectations.\n",
      "‚úÖ Expected Reply:  Thanks for the great feedback! We are glad the quality and fit met your expectations.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Passed Sample 7\n",
      "üìù Review: The FIta or belt is thin because the price is already good.\n",
      "üí¨ Predicted Sentiment: neutral | Expected: neutral\n",
      "üìù Generated Reply: Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your expectations. We are always working to improve, so feel free to check out our other models.\n",
      "‚úÖ Expected Reply:  Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your expectations. We are always working to improve, so feel free to check out our other models.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîÅ Passed Sample 8\n",
      "üìù Review: Delivery Latvia 10 days. Original case, sound, aplication good\n",
      "üí¨ Predicted Sentiment: neutral | Expected: positive\n",
      "üìù Generated Reply: Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your full expectations. We are always working to improve, so feel free to check out our other models.\n",
      "‚úÖ Expected Reply:  Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your full expectations. We are always working to improve, so feel free to check out our other models.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    }
   ],
   "source": [
    "# Step 3 & 4 storage\n",
    "sentiment_preds, sentiment_labels = [], []\n",
    "reply_preds, reply_labels = [], []\n",
    "\n",
    "# ‚úÖ Load sentiment and reply models ONCE\n",
    "model_1, tokenizer_1 = load_unsloth_model(\"AbuSalehMd/Review_Response_Generation_Mistral_7B_FineTuned\")\n",
    "model_2, tokenizer_2 = load_unsloth_model(\"AbuSalehMd/Sentiment_Analysis_Mistral_7B_FineTuned\")\n",
    "\n",
    "print(\"\\nüìã Step 3 & 4 Predictions (for passed samples):\")\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "\n",
    "for idx, item in enumerate(passed_samples):\n",
    "    review = item[\"review\"]\n",
    "    category = item[\"category\"]\n",
    "\n",
    "    # Step 3: Sentiment\n",
    "    sentiment = predict_sentiment(review, model_2, tokenizer_2)\n",
    "    sentiment_preds.append(sentiment)\n",
    "    sentiment_labels.append(item[\"expected_sentiment\"])\n",
    "\n",
    "    # Step 4: Reply\n",
    "    reply = generate_review_reply(review, sentiment, category, model_1, tokenizer_1, alpaca_prompt)\n",
    "    reply_preds.append(reply)\n",
    "    reply_labels.append(item[\"expected_reply\"])\n",
    "\n",
    "    # üìä Print outcomes\n",
    "    print(f\"\\nüîÅ Passed Sample {idx+1}\")\n",
    "    print(f\"üìù Review: {review}\")\n",
    "    print(f\"üí¨ Predicted Sentiment: {sentiment} | Expected: {item['expected_sentiment']}\")\n",
    "    print(f\"üìù Generated Reply: {reply}\")\n",
    "    print(f\"‚úÖ Expected Reply:  {item['expected_reply']}\")\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "\n",
    "# ‚úÖ Unload models after loop\n",
    "del model_1, model_2, tokenizer_1, tokenizer_2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Output Inspection\n",
    "\n",
    "- Calculates **accuracy** and **F1 score** for fake review detection, product category classification, and sentiment analysis.\n",
    "- Evaluates reply generation using **ROUGE**, **METEOR**, and **BERTScore** for natural language quality.\n",
    "- Displays detailed results including predicted sentiment, generated reply, and comparison with expected values.\n",
    "- Reports how many reviews passed all pipeline stages and the overall correct rate against expected valid samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:38:22.410767Z",
     "iopub.status.busy": "2025-04-16T08:38:22.410501Z",
     "iopub.status.idle": "2025-04-16T08:38:24.874775Z",
     "shell.execute_reply": "2025-04-16T08:38:24.873859Z",
     "shell.execute_reply.started": "2025-04-16T08:38:22.410750Z"
    },
    "id": "vps14jvJkWMC",
    "outputId": "2ed3ad27-7a9a-426e-e01e-35e94172a0ec",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Fake Review Detection\n",
      "Accuracy: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "üè∑Ô∏è Category Classification\n",
      "Accuracy: 0.8888888888888888\n",
      "\n",
      "üí¨ Sentiment Classification\n",
      "Accuracy: 0.875\n",
      "F1 Score: 0.7948717948717948\n",
      "\n",
      "‚úçÔ∏è Reply Generation\n",
      "ROUGE: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "METEOR: {'meteor': 0.950050609449925}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: {'precision': [1.000000238418579, 0.9999998807907104, 0.9999998211860657, 0.9999998807907104, 0.937644362449646, 0.9999999403953552, 1.0, 0.9999998211860657], 'recall': [1.000000238418579, 0.9999998807907104, 0.9999998211860657, 0.9999998807907104, 0.978985071182251, 0.9999999403953552, 1.0, 0.9999998211860657], 'f1': [1.000000238418579, 0.9999998807907104, 0.9999998211860657, 0.9999998807907104, 0.9578688740730286, 0.9999999403953552, 1.0, 0.9999998211860657], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.51.1)'}\n",
      "\n",
      "‚úÖ Full Pipeline Passed: 8/14 = 57.14%\n",
      "\n",
      "‚úÖ Full Pipeline Correct Rate: 8/10 = 80.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "meteor = load(\"meteor\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "print(\"\\nüîç Fake Review Detection\")\n",
    "print(\"Accuracy:\", accuracy_score(fake_labels, fake_preds))\n",
    "print(\"F1 Score:\", f1_score(fake_labels, fake_preds, pos_label=\"fake\"))\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Category Classification\")\n",
    "print(\"Accuracy:\", accuracy_score(cat_labels, cat_preds))\n",
    "\n",
    "if sentiment_preds:\n",
    "    print(\"\\nüí¨ Sentiment Classification\")\n",
    "    print(\"Accuracy:\", accuracy_score(sentiment_labels, sentiment_preds))\n",
    "    print(\"F1 Score:\", f1_score(sentiment_labels, sentiment_preds, average=\"macro\"))\n",
    "\n",
    "if reply_preds:\n",
    "    print(\"\\n‚úçÔ∏è Reply Generation\")\n",
    "    print(\"ROUGE:\", rouge.compute(predictions=reply_preds, references=reply_labels))\n",
    "    print(\"METEOR:\", meteor.compute(predictions=reply_preds, references=reply_labels))\n",
    "    print(\"BERTScore:\", bertscore.compute(predictions=reply_preds, references=reply_labels, lang=\"en\"))\n",
    "\n",
    "print(f\"\\n‚úÖ Full Pipeline Passed: {len(passed_samples)}/{len(test_data)} = {len(passed_samples)/len(test_data):.2%}\")\n",
    "print(f\"\\n‚úÖ Full Pipeline Correct Rate: {len(passed_samples)}/{passed_examples} = {len(passed_samples)/passed_examples:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:38:24.876336Z",
     "iopub.status.busy": "2025-04-16T08:38:24.876115Z",
     "iopub.status.idle": "2025-04-16T08:38:24.881856Z",
     "shell.execute_reply": "2025-04-16T08:38:24.881036Z",
     "shell.execute_reply.started": "2025-04-16T08:38:24.876317Z"
    },
    "id": "Jqo_PgBukWMC",
    "outputId": "50809dd4-2e09-453e-c961-f520044cd9d6",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Inspection of Passed Samples with Predictions:\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÅ Review: Very satisfied with the product, it is really quite strong...\n",
      "üè∑Ô∏è  Input Category: automotive\n",
      "üí¨ Predicted Sentiment: positive\n",
      "üí¨ Expected Sentiment:  positive\n",
      "üìù Generated Reply:     That's great to hear! Strong and reliable products are always appreciated. Thanks for the positive note!\n",
      "‚úÖ Expected Reply:      That's great to hear! Strong and reliable products are always appreciated. Thanks for the positive note!\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÅ Review: I really liked I can take out my quiet dog\n",
      "üè∑Ô∏è  Input Category: fashion\n",
      "üí¨ Predicted Sentiment: positive\n",
      "üí¨ Expected Sentiment:  positive\n",
      "üìù Generated Reply:     That's great to hear! Thanks for the positive note and your creative use of the product.\n",
      "‚úÖ Expected Reply:      That's great to hear! Thanks for the positive note and your creative use of the product.\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÅ Review: These seat covers fit perfectly in my car...\n",
      "üè∑Ô∏è  Input Category: automotive\n",
      "üí¨ Predicted Sentiment: positive\n",
      "üí¨ Expected Sentiment:  positive\n",
      "üìù Generated Reply:     Perfect fit is always a good sign! Thanks for the great review and your support.\n",
      "‚úÖ Expected Reply:      Perfect fit is always a good sign! Thanks for the great review and your support.\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÅ Review: Really high-quality bags, buying worth...\n",
      "üè∑Ô∏è  Input Category: fashion\n",
      "üí¨ Predicted Sentiment: positive\n",
      "üí¨ Expected Sentiment:  positive\n",
      "üìù Generated Reply:     That's great to hear! Thanks for the positive note and your support.\n",
      "‚úÖ Expected Reply:      That's great to hear! Thanks for the positive note and your support.\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÅ Review: Order received successfully and faster than expected...\n",
      "üè∑Ô∏è  Input Category: electronics\n",
      "üí¨ Predicted Sentiment: positive\n",
      "üí¨ Expected Sentiment:  positive\n",
      "üìù Generated Reply:     That¬ís always great news¬óthanks for the note!\n",
      "‚úÖ Expected Reply:      That‚Äôs always great news‚Äîthanks for the note!\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÅ Review: I like Xro for my sofa no m served quality fabric\n",
      "üè∑Ô∏è  Input Category: home\n",
      "üí¨ Predicted Sentiment: positive\n",
      "üí¨ Expected Sentiment:  positive\n",
      "üìù Generated Reply:     Thanks for the great feedback! We are glad the quality and fit met your expectations.\n",
      "‚úÖ Expected Reply:      Thanks for the great feedback! We are glad the quality and fit met your expectations.\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÅ Review: The FIta or belt is thin because the price is already good.\n",
      "üè∑Ô∏è  Input Category: health\n",
      "üí¨ Predicted Sentiment: neutral\n",
      "üí¨ Expected Sentiment:  neutral\n",
      "üìù Generated Reply:     Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your expectations. We are always working to improve, so feel free to check out our other models.\n",
      "‚úÖ Expected Reply:      Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your expectations. We are always working to improve, so feel free to check out our other models.\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÅ Review: Delivery Latvia 10 days. Original case, sound, aplication good\n",
      "üè∑Ô∏è  Input Category: electronics\n",
      "üí¨ Predicted Sentiment: neutral\n",
      "üí¨ Expected Sentiment:  positive\n",
      "üìù Generated Reply:     Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your full expectations. We are always working to improve, so feel free to check out our other models.\n",
      "‚úÖ Expected Reply:      Thank you for your feedback! We appreciate your input, and we understand that the product didn't meet your full expectations. We are always working to improve, so feel free to check out our other models.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã Inspection of Passed Samples with Predictions:\\n\" + \"-\"*50)\n",
    "for i, sample in enumerate(passed_samples):\n",
    "    print(f\"\\nüîÅ Review: {sample['review']}\")\n",
    "    print(f\"üè∑Ô∏è  Input Category: {sample['category']}\")\n",
    "    print(f\"üí¨ Predicted Sentiment: {sentiment_preds[i]}\")\n",
    "    print(f\"üí¨ Expected Sentiment:  {sentiment_labels[i]}\")\n",
    "    print(f\"üìù Generated Reply:     {reply_preds[i]}\")\n",
    "    print(f\"‚úÖ Expected Reply:      {reply_labels[i]}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiOmB_bZkWMG"
   },
   "source": [
    "# Full pipeline (Not Worked)\n",
    "\n",
    "- The device can't load the 4 models in onece.\n",
    "- It gives error on loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:58:25.496763Z",
     "iopub.status.busy": "2025-04-15T17:58:25.496166Z",
     "iopub.status.idle": "2025-04-15T17:59:10.552622Z",
     "shell.execute_reply": "2025-04-15T17:59:10.551789Z",
     "shell.execute_reply.started": "2025-04-15T17:58:25.496734Z"
    },
    "id": "sFB5oO0pkWMG",
    "outputId": "7a5c3780-52f5-41fe-d797-f2eda2980dbc",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:58:42.146684: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744739922.562972      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744739922.684205      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Global config\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "# Alpaca prompt template for reply generation\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Load model on specific device\n",
    "def load_unsloth_model(model_name, device):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zQZjv7HkWMG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_fake, tokenizer_fake = load_unsloth_model(\"AbuSalehMd/FakeReviewDetection_Mistral_7B_FineTuned\", device=\"cuda:0\")\n",
    "model_category, tokenizer_category = load_unsloth_model(\"AbuSalehMd/ProductCategoryClassificationFinal_Mistral_7B_FineTuned\", device=\"cuda:0\")\n",
    "model_sentiment, tokenizer_sentiment = load_unsloth_model(\"AbuSalehMd/Sentiment_Analysis_Mistral_7B_FineTuned\", device=\"cuda:0\")\n",
    "model_reply, tokenizer_reply = load_unsloth_model(\"AbuSalehMd/Review_Response_Generation_Mistral_7B_FineTuned\", device=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:59:10.554312Z",
     "iopub.status.busy": "2025-04-15T17:59:10.554025Z",
     "iopub.status.idle": "2025-04-15T17:59:10.56465Z",
     "shell.execute_reply": "2025-04-15T17:59:10.564053Z",
     "shell.execute_reply.started": "2025-04-15T17:59:10.554286Z"
    },
    "id": "sX6bFnJPkWMG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---- INFERENCE FUNCTIONS ---- #\n",
    "\n",
    "def predict_fake_review(review, model, tokenizer):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    prompt = f\"\"\"\n",
    "    Determine if the review enclosed in square brackets is real or fake based on its content.\n",
    "    Return the answer as either \"real\" or \"fake\".\n",
    "\n",
    "    [{review}] =\n",
    "    \"\"\".strip()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1, temperature=0.0)\n",
    "    answer = tokenizer.decode(outputs[0]).split(\"=\")[-1].strip().lower()\n",
    "    return \"real\" if \"real\" in answer else \"fake\" if \"fake\" in answer else \"none\"\n",
    "\n",
    "def predict_category(review, model, tokenizer):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    prompt = f\"\"\"\n",
    "    Determine the class if the review enclosed in square brackets is automotive or fashion or home or electronics or health category class based on its content.\n",
    "    Return the answer as either \"automotive\" or \"fashion\" or \"home\" or \"electronics\" or \"health\".\n",
    "\n",
    "    [{review}] =\n",
    "    \"\"\".strip()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1, temperature=0.0)\n",
    "    answer = tokenizer.decode(outputs[0]).split(\"=\")[-1].strip().lower()\n",
    "\n",
    "    if \"autom\" in answer:\n",
    "        return \"automotive\"\n",
    "    elif \"fashion\" in answer:\n",
    "        return \"fashion\"\n",
    "    elif \"home\" in answer:\n",
    "        return \"home\"\n",
    "    elif \"electron\" in answer:\n",
    "        return \"electronics\"\n",
    "    elif \"health\" in answer:\n",
    "        return \"health\"\n",
    "    else:\n",
    "        return \"none\"\n",
    "\n",
    "def predict_sentiment(review, model, tokenizer):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    prompt = f\"\"\"\n",
    "    Determine if the review enclosed in square brackets is positive, neutral or negative based on its content.\n",
    "    Return the answer as either \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "    [{review}] =\n",
    "    \"\"\".strip()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1, temperature=0.0)\n",
    "    answer = tokenizer.decode(outputs[0]).split(\"=\")[-1].strip().lower()\n",
    "    return answer if answer in [\"positive\", \"neutral\", \"negative\"] else \"none\"\n",
    "\n",
    "def generate_review_reply(review, sentiment, category, model, tokenizer, prompt_template, device):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    prompt = prompt_template.format(\n",
    "        \"Generate a helpful and context-aware reply based on the review, sentiment, and category.\",\n",
    "        f\"Review: {review}\\nSentiment: {sentiment}\\nCategory: {category}\",\n",
    "        \"\"\n",
    "    )\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"### Response:\")[-1].strip() if \"### Response:\" in decoded else decoded.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:59:10.565441Z",
     "iopub.status.busy": "2025-04-15T17:59:10.56525Z",
     "iopub.status.idle": "2025-04-15T17:59:10.591871Z",
     "shell.execute_reply": "2025-04-15T17:59:10.591324Z",
     "shell.execute_reply.started": "2025-04-15T17:59:10.565426Z"
    },
    "id": "3pYNgFiekWMG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---- MAIN PIPELINE ---- #\n",
    "\n",
    "def full_pipeline(review, input_category):\n",
    "    print(f\"üîç Review: {review}\\nüì¶ Input Category: {input_category}\")\n",
    "\n",
    "    # Step 1: Fake Review Detection\n",
    "    fake_result = predict_fake_review(review, model_fake, tokenizer_fake)\n",
    "    print(\"üïµÔ∏è Fake Review Detection:\", fake_result)\n",
    "\n",
    "    if fake_result == \"fake\":\n",
    "        return \"‚ùå Detected as fake review.\"\n",
    "\n",
    "    # Step 2: Category Classification\n",
    "    category = predict_category(review, model_category, tokenizer_category)\n",
    "    print(\"üè∑Ô∏è Predicted Category:\", category)\n",
    "\n",
    "    if category != input_category.lower():\n",
    "        return \"‚ö†Ô∏è Irrelevant category.\"\n",
    "\n",
    "    # Step 3: Sentiment Classification\n",
    "    sentiment = predict_sentiment(review, model_sentiment, tokenizer_sentiment)\n",
    "    print(\"üí¨ Sentiment:\", sentiment)\n",
    "\n",
    "    # Step 4: Generate Reply\n",
    "    reply = generate_review_reply(review, sentiment, category, model_reply, tokenizer_reply, alpaca_prompt, device=\"cuda:0\")\n",
    "    print(\"‚úçÔ∏è Generated Reply:\")\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T18:01:29.720855Z",
     "iopub.status.busy": "2025-04-15T18:01:29.720526Z",
     "iopub.status.idle": "2025-04-15T18:01:31.566074Z",
     "shell.execute_reply": "2025-04-15T18:01:31.564706Z",
     "shell.execute_reply.started": "2025-04-15T18:01:29.720832Z"
    },
    "id": "olSjvrpQkWMH",
    "outputId": "467950c3-1f12-43b7-8dcc-d566d6ad4c09",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Review: I do not understand the enthusiastic reviews on the basis of which I ordered this bag. Looks very cheap. As the skin of a young dermatine is said.\n",
      "üì¶ Input Category: health\n",
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 6397 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 9.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/886212920.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"health\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_category\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/65796558.py\u001b[0m in \u001b[0;36mfull_pipeline\u001b[0;34m(review, input_category)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Step 1: Fake Review Detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_unsloth_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AbuSalehMd/FakeReviewDetection_Mistral_7B_FineTuned\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfake_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fake_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0munload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/1777785069.py\u001b[0m in \u001b[0;36mload_unsloth_model\u001b[0;34m(model_name, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Load model on specific device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_unsloth_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/mistral.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     ):\n\u001b[0;32m--> 400\u001b[0;31m         return FastLlamaModel.from_pretrained(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfast_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4340\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4341\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4342\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4344\u001b[0m         \u001b[0;31m# Make sure to tie the weights correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMistralModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    443\u001b[0m         )\n\u001b[1;32m    444\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMistralRMSNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrms_norm_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotary_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMistralRotaryEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim, max_position_embeddings, base, device, config)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;31m# Build here to make `torch.jit.trace` work.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_cos_sin_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_rope_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_set_cos_sin_cache\u001b[0;34m(self, seq_len, device, dtype)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;31m# Different from paper, but it uses a different permutation in order to obtain the same calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cos_cached\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sin_cached\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 6397 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 9.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ---- EXAMPLE USAGE ---- #\n",
    "\n",
    "review = \"I do not understand the enthusiastic reviews on the basis of which I ordered this bag. Looks very cheap. As the skin of a young dermatine is said.\"\n",
    "input_category = \"health\"\n",
    "\n",
    "output = full_pipeline(review, input_category)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LLM based Review Management System Pipeline",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
